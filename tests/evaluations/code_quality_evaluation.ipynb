{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Quality Evaluation Framework\n",
    "\n",
    "This notebook implements a comprehensive evaluation framework for assessing the quality of code generated by the Paper2Code agent system.\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "The evaluation framework assesses code quality across multiple dimensions:\n",
    "\n",
    "1. **Code Quality Metrics**\n",
    "   - Readability and maintainability\n",
    "   - Code complexity\n",
    "   - Documentation coverage\n",
    "   - Test coverage\n",
    "\n",
    "2. **Functional Correctness**\n",
    "   - Algorithm accuracy\n",
    "   - Implementation correctness\n",
    "   - Edge case handling\n",
    "\n",
    "3. **Performance Metrics**\n",
    "   - Execution time\n",
    "   - Memory usage\n",
    "   - Scalability\n",
    "\n",
    "4. **Research Fidelity**\n",
    "   - Paper-to-code accuracy\n",
    "   - Algorithm completeness\n",
    "   - Experimental reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import ast\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add the app directory to the Python path\n",
    "app_dir = Path().cwd().parent / \"app\"\n",
    "sys.path.insert(0, str(app_dir))\n",
    "\n",
    "# Import evaluation modules\n",
    "from app.models.code import CodeImplementation, CodeFile, Language\n",
    "from app.models.paper import Paper, PaperMetadata, Author\n",
    "from app.agents import QualityAssuranceAgent, ValidationResult\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Evaluation framework initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Quality Metrics Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeQualityMetrics:\n",
    "    \"\"\"Calculate various code quality metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def calculate_readability_score(self, code: str) -> float:\n",
    "        \"\"\"Calculate code readability score (0-1)\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate various readability indicators\n",
    "        lines = code.split('\\n')\n",
    "        total_lines = len(lines)\n",
    "        \n",
    "        # Comment density\n",
    "        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "        comment_density = comment_lines / total_lines if total_lines > 0 else 0\n",
    "        \n",
    "        # Average line length\n",
    "        avg_line_length = np.mean([len(line) for line in lines if line.strip()]) if any(line.strip() for line in lines) else 0\n",
    "        \n",
    "        # Function/variable name length\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            function_names = []\n",
    "            variable_names = []\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    function_names.append(node.name)\n",
    "                elif isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n",
    "                    variable_names.append(node.id)\n",
    "            \n",
    "            avg_name_length = np.mean([len(name) for name in function_names + variable_names]) if function_names + variable_names else 0\n",
    "        except:\n",
    "            avg_name_length = 0\n",
    "        \n",
    "        # Calculate readability score (0-1)\n",
    "        readability_score = (\n",
    "            min(comment_density * 2, 1.0) * 0.3 +  # Comments are good\n",
    "            min(1.0 - (avg_line_length / 100), 1.0) * 0.3 +  # Shorter lines are better\n",
    "            min(avg_name_length / 15, 1.0) * 0.4  # Reasonable name lengths\n",
    "        )\n",
    "        \n",
    "        return max(0.0, min(1.0, readability_score))\n",
    "    \n",
    "    def calculate_complexity_score(self, code: str) -> float:\n",
    "        \"\"\"Calculate code complexity score (0-1, lower is better)\"\"\"\n",
    "        if not code:\n",
    "            return 1.0\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Cyclomatic complexity\n",
    "            complexity = 1  # Base complexity\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):\n",
    "                    complexity += 1\n",
    "                elif isinstance(node, ast.ExceptHandler):\n",
    "                    complexity += 1\n",
    "                elif isinstance(node, ast.BoolOp):\n",
    "                    complexity += len(node.values) - 1\n",
    "            \n",
    "            # Normalize complexity (0-1, lower is better)\n",
    "            normalized_complexity = min(complexity / 20, 1.0)\n",
    "            \n",
    "            return normalized_complexity\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating complexity: {e}\")\n",
    "            return 1.0\n",
    "    \n",
    "    def calculate_documentation_score(self, code: str) -> float:\n",
    "        \"\"\"Calculate documentation coverage score (0-1)\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            total_functions = 0\n",
    "            documented_functions = 0\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    total_functions += 1\n",
    "                    \n",
    "                    # Check for docstring\n",
    "                    if (node.body and \n",
    "                        isinstance(node.body[0], ast.Expr) and \n",
    "                        isinstance(node.body[0].value, ast.Constant) and \n",
    "                        isinstance(node.body[0].value.value, str)):\n",
    "                        documented_functions += 1\n",
    "            \n",
    "            # Check for module-level docstring\n",
    "            has_module_docstring = False\n",
    "            if tree.body and isinstance(tree.body[0], ast.Expr):\n",
    "                if isinstance(tree.body[0].value, ast.Constant) and isinstance(tree.body[0].value.value, str):\n",
    "                    has_module_docstring = True\n",
    "            \n",
    "            # Calculate documentation score\n",
    "            function_doc_score = documented_functions / total_functions if total_functions > 0 else 0\n",
    "            module_doc_score = 1.0 if has_module_docstring else 0.0\n",
    "            \n",
    "            return (function_doc_score * 0.7 + module_doc_score * 0.3)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating documentation score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_test_coverage_score(self, code: str, test_code: str) -> float:\n",
    "        \"\"\"Calculate test coverage score (0-1)\"\"\"\n",
    "        if not code or not test_code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Extract function names from main code\n",
    "            main_tree = ast.parse(code)\n",
    "            main_functions = set()\n",
    "            \n",
    "            for node in ast.walk(main_tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    main_functions.add(node.name)\n",
    "            \n",
    "            # Check if test code references these functions\n",
    "            test_tree = ast.parse(test_code)\n",
    "            tested_functions = set()\n",
    "            \n",
    "            for node in ast.walk(test_tree):\n",
    "                if isinstance(node, ast.Call) and isinstance(node.func, ast.Name):\n",
    "                    if node.func.id in main_functions:\n",
    "                        tested_functions.add(node.func.id)\n",
    "            \n",
    "            # Calculate coverage score\n",
    "            coverage = len(tested_functions) / len(main_functions) if main_functions else 0\n",
    "            \n",
    "            return coverage\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error calculating test coverage: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_maintainability_score(self, code: str) -> float:\n",
    "        \"\"\"Calculate maintainability score (0-1)\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        # Combine various metrics into maintainability score\n",
    "        readability = self.calculate_readability_score(code)\n",
    "        complexity = 1.0 - self.calculate_complexity_score(code)  # Invert complexity\n",
    "        documentation = self.calculate_documentation_score(code)\n",
    "        \n",
    "        # Weighted average\n",
    "        maintainability = (\n",
    "            readability * 0.4 +\n",
    "            complexity * 0.3 +\n",
    "            documentation * 0.3\n",
    "        )\n",
    "        \n",
    "        return maintainability\n",
    "    \n",
    "    def calculate_all_metrics(self, code: str, test_code: str = \"\") -> Dict[str, float]:\n",
    "        \"\"\"Calculate all code quality metrics\"\"\"\n",
    "        metrics = {\n",
    "            'readability': self.calculate_readability_score(code),\n",
    "            'complexity': self.calculate_complexity_score(code),\n",
    "            'documentation': self.calculate_documentation_score(code),\n",
    "            'test_coverage': self.calculate_test_coverage_score(code, test_code),\n",
    "            'maintainability': self.calculate_maintainability_score(code)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"✅ Code quality metrics calculator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional Correctness Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionalCorrectnessEvaluator:\n",
    "    \"\"\"Evaluate functional correctness of generated code\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_results = {}\n",
    "    \n",
    "    def evaluate_algorithm_accuracy(self, code: str, test_cases: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"Evaluate algorithm accuracy against test cases\"\"\"\n",
    "        if not code or not test_cases:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Create a temporary directory for testing\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Write the code to a temporary file\n",
    "                code_file = Path(temp_dir) / \"algorithm.py\"\n",
    "                with open(code_file, 'w') as f:\n",
    "                    f.write(code)\n",
    "                \n",
    "                # Run test cases\n",
    "                passed_tests = 0\n",
    "                total_tests = len(test_cases)\n",
    "                \n",
    "                for test_case in test_cases:\n",
    "                    try:\n",
    "                        # Create test script\n",
    "                        test_script = f\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, '{temp_dir}')\n",
    "from algorithm import {test_case.get('function_name', 'main')}\n",
    "\n",
    "# Test case\n",
    "result = {test_case.get('function_name', 'main')}({test_case.get('input', '')})\n",
    "expected = {test_case.get('expected', '')}\n",
    "\n",
    "if result == expected:\n",
    "    print(\"PASS\")\n",
    "else:\n",
    "    print(f\"FAIL: Expected {expected}, got {result}\")\n",
    "    sys.exit(1)\n",
    "\"\"\"\n",
    "                        \n",
    "                        # Run the test\n",
    "                        result = subprocess.run(\n",
    "                            [sys.executable, '-c', test_script],\n",
    "                            capture_output=True,\n",
    "                            text=True,\n",
    "                            timeout=30\n",
    "                        )\n",
    "                        \n",
    "                        if result.returncode == 0:\n",
    "                            passed_tests += 1\n",
    "                        else:\n",
    "                            logger.warning(f\"Test failed: {result.stderr}\")\n",
    "                            \n",
    "                    except subprocess.TimeoutExpired:\n",
    "                        logger.warning(\"Test timed out\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Test error: {e}\")\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                accuracy = passed_tests / total_tests if total_tests > 0 else 0\n",
    "                return accuracy\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating algorithm accuracy: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_implementation_correctness(self, code: str, expected_patterns: List[str]) -> float:\n",
    "        \"\"\"Evaluate if implementation follows expected patterns\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Check for expected patterns in the code\n",
    "            matched_patterns = 0\n",
    "            total_patterns = len(expected_patterns)\n",
    "            \n",
    "            for pattern in expected_patterns:\n",
    "                if pattern.lower() in code.lower():\n",
    "                    matched_patterns += 1\n",
    "            \n",
    "            # Calculate pattern match score\n",
    "            pattern_score = matched_patterns / total_patterns if total_patterns > 0 else 0\n",
    "            \n",
    "            return pattern_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating implementation correctness: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_edge_case_handling(self, code: str) -> float:\n",
    "        \"\"\"Evaluate edge case handling in the code\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Check for edge case handling patterns\n",
    "            edge_case_patterns = [\n",
    "                'try:',\n",
    "                'except',\n",
    "                'if.*is.*None',\n",
    "                'if.*len.*==.*0',\n",
    "                'raise.*Exception',\n",
    "                'assert',\n",
    "                'validate',\n",
    "                'check'\n",
    "            ]\n",
    "            \n",
    "            matched_patterns = 0\n",
    "            total_patterns = len(edge_case_patterns)\n",
    "            \n",
    "            for pattern in edge_case_patterns:\n",
    "                if re.search(pattern, code, re.IGNORECASE):\n",
    "                    matched_patterns += 1\n",
    "            \n",
    "            # Calculate edge case handling score\n",
    "            edge_case_score = matched_patterns / total_patterns if total_patterns > 0 else 0\n",
    "            \n",
    "            return edge_case_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating edge case handling: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_functional_correctness(self, code: str, test_cases: List[Dict[str, Any]], \n",
    "                                      expected_patterns: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate overall functional correctness\"\"\"\n",
    "        results = {\n",
    "            'algorithm_accuracy': self.evaluate_algorithm_accuracy(code, test_cases),\n",
    "            'implementation_correctness': self.evaluate_implementation_correctness(code, expected_patterns),\n",
    "            'edge_case_handling': self.evaluate_edge_case_handling(code)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall functional correctness score\n",
    "        results['overall'] = np.mean(list(results.values()))\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ Functional correctness evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceEvaluator:\n",
    "    \"\"\"Evaluate performance metrics of generated code\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.performance_results = {}\n",
    "    \n",
    "    def evaluate_execution_time(self, code: str, test_input: Any, iterations: int = 10) -> float:\n",
    "        \"\"\"Evaluate execution time of the code\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Write the code to a temporary file\n",
    "                code_file = Path(temp_dir) / \"performance_test.py\"\n",
    "                with open(code_file, 'w') as f:\n",
    "                    f.write(code)\n",
    "                \n",
    "                # Create performance test script\n",
    "                test_script = f\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, '{temp_dir}')\n",
    "import time\n",
    "from algorithm import {test_input.get('function_name', 'main')}\n",
    "\n",
    "# Warm up\n",
    "for _ in range(5):\n",
    "    {test_input.get('function_name', 'main')}({test_input.get('input', '')})\n",
    "\n",
    "# Measure execution time\n",
    "times = []\n",
    "for _ in range({iterations}):\n",
    "    start_time = time.time()\n",
    "    {test_input.get('function_name', 'main')}({test_input.get('input', '')})\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "# Calculate average execution time\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"{avg_time:.6f}\")\n",
    "\"\"\"\n",
    "                \n",
    "                # Run the performance test\n",
    "                result = subprocess.run(\n",
    "                    [sys.executable, '-c', test_script],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=60\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    execution_time = float(result.stdout.strip())\n",
    "                    return execution_time\n",
    "                else:\n",
    "                    logger.warning(f\"Performance test failed: {result.stderr}\")\n",
    "                    return 0.0\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating execution time: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_memory_usage(self, code: str, test_input: Any) -> float:\n",
    "        \"\"\"Evaluate memory usage of the code\"\"\"\n",
    "        if not code:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                # Write the code to a temporary file\n",
    "                code_file = Path(temp_dir) / \"memory_test.py\"\n",
    "                with open(code_file, 'w') as f:\n",
    "                    f.write(code)\n",
    "                \n",
    "                # Create memory test script\n",
    "                test_script = f\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, '{temp_dir}')\n",
    "import tracemalloc\n",
    "from algorithm import {test_input.get('function_name', 'main')}\n",
    "\n",
    "# Start memory tracing\n",
    "tracemalloc.start()\n",
    "\n",
    "# Run the function\n",
    "for _ in range(10):\n",
    "    {test_input.get('function_name', 'main')}({test_input.get('input', '')})\n",
    "\n",
    "# Get memory usage\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Print peak memory usage in MB\n",
    "print(f\"{peak / 1024 / 1024:.2f}\")\n",
    "\"\"\"\n",
    "                \n",
    "                # Run the memory test\n",
    "                result = subprocess.run(\n",
    "                    [sys.executable, '-c', test_script],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=60\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    memory_usage = float(result.stdout.strip())\n",
    "                    return memory_usage\n",
    "                else:\n",
    "                    logger.warning(f\"Memory test failed: {result.stderr}\")\n",
    "                    return 0.0\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating memory usage: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_scalability(self, code: str, test_inputs: List[Any]) -> float:\n",
    "        \"\"\"Evaluate scalability of the code\"\"\"\n",
    "        if not code or not test_inputs:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            execution_times = []\n",
    "            input_sizes = []\n",
    "            \n",
    "            for test_input in test_inputs:\n",
    "                execution_time = self.evaluate_execution_time(code, test_input, iterations=5)\n",
    "                if execution_time > 0:\n",
    "                    execution_times.append(execution_time)\n",
    "                    input_sizes.append(test_input.get('size', len(test_input.get('input', []))))\n",
    "            \n",
    "            if len(execution_times) < 2:\n",
    "                return 0.0\n",
    "            \n",
    "            # Calculate scalability score (how well execution time scales with input size)\n",
    "            # Lower score means better scalability (sub-linear growth)\n",
    "            if len(execution_times) > 1:\n",
    "                # Calculate growth rate\n",
    "                growth_rates = []\n",
    "                for i in range(1, len(execution_times)):\n",
    "                    size_ratio = input_sizes[i] / input_sizes[i-1]\n",
    "                    time_ratio = execution_times[i] / execution_times[i-1]\n",
    "                    growth_rates.append(time_ratio / size_ratio)\n",
    "                \n",
    "                # Average growth rate (lower is better)\n",
    "                avg_growth_rate = np.mean(growth_rates)\n",
    "                \n",
    "                # Convert to score (0-1, higher is better)\n",
    "                scalability_score = 1.0 / (1.0 + avg_growth_rate)\n",
    "                return scalability_score\n",
    "            \n",
    "            return 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating scalability: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_performance(self, code: str, test_input: Any, \n",
    "                            test_inputs: List[Any] = None) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate overall performance metrics\"\"\"\n",
    "        if test_inputs is None:\n",
    "            test_inputs = [test_input]\n",
    "        \n",
    "        results = {\n",
    "            'execution_time': self.evaluate_execution_time(code, test_input),\n",
    "            'memory_usage': self.evaluate_memory_usage(code, test_input),\n",
    "            'scalability': self.evaluate_scalability(code, test_inputs)\n",
    "        }\n",
    "        \n",
    "        # Normalize performance metrics (0-1, higher is better)\n",
    "        # Execution time: inverse (faster is better)\n",
    "        if results['execution_time'] > 0:\n",
    "            results['execution_time'] = 1.0 / (1.0 + results['execution_time'])\n",
    "        \n",
    "        # Memory usage: inverse (less is better)\n",
    "        if results['memory_usage'] > 0:\n",
    "            results['memory_usage'] = 1.0 / (1.0 + results['memory_usage'] / 100)  # Normalize to MB\n",
    "        \n",
    "        # Calculate overall performance score\n",
    "        results['overall'] = np.mean([results['execution_time'], results['memory_usage'], results['scalability']])\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"✅ Performance evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Fidelity Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchFidelityEvaluator:\n",
    "    \"\"\"Evaluate how well the generated code matches the original research\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fidelity_results = {}\n",
    "    \n",
    "    def evaluate_algorithm_completeness(self, code: str, paper_algorithms: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"Evaluate if all algorithms from the paper are implemented\"\"\"\n",
    "        if not code or not paper_algorithms:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Extract algorithm names from paper\n",
    "            paper_algorithm_names = [algo.get('name', '').lower() for algo in paper_algorithms]\n",
    "            \n",
    "            # Check if code contains references to these algorithms\n",
    "            code_lower = code.lower()\n",
    "            \n",
    "            matched_algorithms = 0\n",
    "            for algo_name in paper_algorithm_names:\",
    "                if algo_name and algo_name in code_lower:\n",
    "                    matched_algorithms += 1\n",
    "            \n",
    "            # Calculate completeness score\n",
    "            completeness = matched_algorithms / len(paper_algorithm_names) if paper_algorithm_names else 0\n",
    "            \n",
    "            return completeness\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating algorithm completeness: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_paper_to_code_accuracy(self, code: str, paper_content: str) -> float:\n",
    "        \"\"\"Evaluate how accurately the code reflects the paper content\"\"\"\n",
    "        if not code or not paper_content:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Extract key terms and concepts from paper\n",
    "            paper_terms = self._extract_key_terms(paper_content)\n",
    "            \n",
    "            # Check if these terms are present in the code\n",
    "            code_terms = self._extract_key_terms(code)\n",
    "            \n",
    "            # Calculate term overlap\n",
    "            if paper_terms and code_terms:\n",
    "                overlap = len(set(paper_terms) & set(code_terms))\n",
    "                total_terms = len(set(paper_terms) | set(code_terms))\n",
    "                accuracy = overlap / total_terms if total_terms > 0 else 0\n",
    "                return accuracy\n",
    "            \n",
    "            return 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating paper-to-code accuracy: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_experimental_reproduction(self, code: str, paper_experiments: List[Dict[str, Any]]) -> float:\n",
    "        \"\"\"Evaluate if the code can reproduce the experiments from the paper\"\"\"\n",
    "        if not code or not paper_experiments:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Check for experiment-related patterns in code\n",
    "            experiment_patterns = [\n",
    "                'experiment',\n",
    "                'evaluation',\n",
    "                'benchmark',\n",
    "                'test',\n",
    "                'dataset',\n",
    "                'metric',\n",
    "                'accuracy',\n",
    "                'precision',\n",
    "                'recall',\n",
    "                'f1',\n",
    "                'bleu',\n",
    "                'loss'\n",
    "            ]\n",
    "            \n",
    "            matched_patterns = 0\n",
    "            total_patterns = len(experiment_patterns)\n",
    "            \n",
    "            code_lower = code.lower()\n",
    "            for pattern in experiment_patterns:\n",
    "                if pattern in code_lower:\n",
    "                    matched_patterns += 1\n",
    "            \n",
    "            # Calculate reproduction score\n",
    "            reproduction_score = matched_patterns / total_patterns if total_patterns > 0 else 0\n",
    "            \n",
    "            return reproduction_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating experimental reproduction: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_research_fidelity(self, code: str, paper_algorithms: List[Dict[str, Any]], \n",
    "                                  paper_content: str, paper_experiments: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate overall research fidelity\"\"\"\n",
    "        results = {\n",
    "            'algorithm_completeness': self.evaluate_algorithm_completeness(code, paper_algorithms),\n",
    "            'paper_to_code_accuracy': self.evaluate_paper_to_code_accuracy(code, paper_content),\n",
    "            'experimental_reproduction': self.evaluate_experimental_reproduction(code, paper_experiments)\n",
    "        }\n",
    "        \n",
    "        # Calculate overall fidelity score\n",
    "        results['overall'] = np.mean(list(results.values()))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _extract_key_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key terms from text\"\"\"\n",
    "        # Simple term extraction - can be enhanced with NLP techniques\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        \n",
    "        # Filter out common words and very short words\n",
    "        stop_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'a', 'an'}\n",
    "        \n",
    "        key_terms = [word for word in words if word not in stop_words and len(word) > 3]\n",
    "        \n",
    "        # Return most frequent terms\n",
    "        term_freq = {}\n",
    "        for term in key_terms:\n",
    "            term_freq[term] = term_freq.get(term, 0) + 1\n",
    "        \n",
    "        # Return top 50 terms\n",
    "        return sorted(term_freq.keys(), key=lambda x: term_freq[x], reverse=True)[:50]\n",
    "\n",
    "print(\"✅ Research fidelity evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Code Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveCodeQualityEvaluator:\n",
    "    \"\"\"Comprehensive code quality evaluation system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.quality_metrics = CodeQualityMetrics()\n",
    "        self.functional_evaluator = FunctionalCorrectnessEvaluator()\n",
    "        self.performance_evaluator = PerformanceEvaluator()\n",
    "        self.research_evaluator = ResearchFidelityEvaluator()\n",
    "        self.evaluation_results = {}\n",
    "    \n",
    "    def evaluate_code_quality(self, code_implementation, paper_data, test_cases=None, \n",
    "                            expected_patterns=None, test_inputs=None):\n",
    "        \"\"\"Perform comprehensive code quality evaluation\"\"\"\n",
    "        if test_cases is None:\n",
    "            test_cases = []\n",
    "        if expected_patterns is None:\n",
    "            expected_patterns = []\n",
    "        if test_inputs is None:\n",
    "            test_inputs = []\n",
    "        \n",
    "        logger.info(\"Starting comprehensive code quality evaluation...\")\n",
    "        \n",
    "        # Extract code and test code\n",
    "        code_files = code_implementation.generated_files if hasattr(code_implementation, 'generated_files') else []\n",
    "        main_code = \"\"\n",
    "        test_code = \"\"\n",
    "        \n",
    "        for file_info in code_files:\n",
    "            if file_info.file_type in ['main', 'algorithm']:\n",
    "                main_code += file_info.content + \"\\n\"\n",
    "            elif file_info.file_type == 'test':\n",
    "                test_code += file_info.content + \"\\n\"\n",
    "        \n",
    "        # Evaluate code quality metrics\n",
    "        logger.info(\"Evaluating code quality metrics...\")\n",
    "        quality_metrics = self.quality_metrics.calculate_all_metrics(main_code, test_code)\n",
    "        \n",
    "        # Evaluate functional correctness\n",
    "        logger.info(\"Evaluating functional correctness...\")\n",
    "        functional_results = self.functional_evaluator.evaluate_functional_correctness(\n",
    "            main_code, test_cases, expected_patterns\n",
    "        )\n",
    "        \n",
    "        # Evaluate performance\n",
    "        logger.info(\"Evaluating performance...\")\n",
    "        if test_inputs:\n",
    "            performance_results = self.performance_evaluator.evaluate_performance(\n",
    "                main_code, test_inputs[0] if test_inputs else {}, test_inputs\n",
    "            )\n",
    "        else:\n",
    "            # Use default test input if none provided\n",
    "            default_test_input = {\n",
    "                'function_name': 'main',\n",
    "                'input': '[]',\n",
    "                'size': 10\n",
    "            }\n",
    "            performance_results = self.performance_evaluator.evaluate_performance(\n",
    "                main_code, default_test_input\n",
    "            )\n",
    "        \n",
    "        # Evaluate research fidelity\n",
    "        logger.info(\"Evaluating research fidelity...\")\n",
    "        research_results = self.research_evaluator.evaluate_research_fidelity(\n",
    "            main_code, \n",
    "            paper_data.algorithms if hasattr(paper_data, 'algorithms') else [],\n",
    "            paper_data.content if hasattr(paper_data, 'content') else \"\",\n",
    "            paper_data.experiments if hasattr(paper_data, 'experiments') else []\n",
    "        )\n",
    "        \n",
    "        # Combine all results\n",
    "        evaluation_results = {\n",
    "            'quality_metrics': quality_metrics,\n",
    "            'functional_correctness': functional_results,\n",
    "            'performance': performance_results,\n",
    "            'research_fidelity': research_results,\n",
    "            'overall_score': self._calculate_overall_score(\n",
    "                quality_metrics, functional_results, performance_results, research_results\n",
    "            ),\n",
    "            'evaluation_timestamp': datetime.now().isoformat(),\n",
    "            'code_summary': {\n",
    "                'total_files': len(code_files),\n",
    "                'main_code_lines': len(main_code.split('\\n')) if main_code else 0,\n",
    "                'test_code_lines': len(test_code.split('\\n')) if test_code else 0,\n",
    "                'language': code_implementation.language_used if hasattr(code_implementation, 'language_used') else 'unknown',\n",
    "                'framework': code_implementation.framework_used if hasattr(code_implementation, 'framework_used') else 'unknown'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results = evaluation_results\n",
    "        logger.info(\"Comprehensive evaluation completed successfully!\")\n",
    "        \n",
    "        return evaluation_results\n",
    "    \n",
    "    def _calculate_overall_score(self, quality_metrics, functional_results, performance_results, research_results):\n",
    "        \"\"\"Calculate overall code quality score\"\"\"\n",
    "        # Weighted average of different evaluation categories\n",
    "        weights = {\n",
    "            'quality_metrics': 0.3,\n",
    "            'functional_correctness': 0.3,\n",
    "            'performance': 0.2,\n",
    "            'research_fidelity': 0.2\n",
    "        }\n",
    "        \n",
    "        # Calculate weighted scores\n",
    "        quality_score = np.mean(list(quality_metrics.values()))\n",
    "        functional_score = functional_results['overall']\n",
    "        performance_score = performance_results['overall']\n",
    "        research_score = research_results['overall']\n",
    "        \n",
    "        overall_score = (\n",
    "            quality_score * weights['quality_metrics'] +\n",
    "            functional_score * weights['functional_correctness'] +\n",
    "            performance_score * weights['performance'] +\n",
    "            research_score * weights['research_fidelity']\n",
    "        )\n",
    "        \n",
    "        return overall_score\n",
    "    \n",
    "    def generate_evaluation_report(self, evaluation_results):\n",
    "        \"\"\"Generate a comprehensive evaluation report\"\"\"\n",
    "        report = {\n",
    "            'evaluation_summary': {\n",
    "                'overall_score': evaluation_results['overall_score'],\n",
    "                'grade': self._get_grade(evaluation_results['overall_score']),\n",
    "                'evaluation_timestamp': evaluation_results['evaluation_timestamp']\n",
    "            },\n",
    "            'code_quality_metrics': evaluation_results['quality_metrics'],\n",
    "            'functional_correctness': evaluation_results['functional_correctness'],\n",
    "            'performance_metrics': evaluation_results['performance'],\n",
    "            'research_fidelity': evaluation_results['research_fidelity'],\n",
    "            'code_summary': evaluation_results['code_summary'],\n",
    "            'recommendations': self._generate_recommendations(evaluation_results),\n",
    "            'strengths': self._identify_strengths(evaluation_results),\n",
    "            'weaknesses': self._identify_weaknesses(evaluation_results)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _get_grade(self, score):\n",
    "        \"\"\"Get letter grade based on score\"\"\"\",
    "        if score >= 0.9:\n",
    "            return 'A+'\n",
    "        elif score >= 0.8:\n",
    "            return 'A'\n",
    "        elif score >= 0.7:\n",
    "            return 'B'\n",
    "        elif score >= 0.6:\n",
    "            return 'C'\n",
    "        elif score >= 0.5:\n",
    "            return 'D'\n",
    "        else:\n",
    "            return 'F'\n",
    "    \n",
    "    def _generate_recommendations(self, evaluation_results):\n",
    "        \"\"\"Generate improvement recommendations\"\"\"\",
    "        recommendations = []\n",
    "        \n",
    "        # Quality metrics recommendations\n",
    "        if evaluation_results['quality_metrics']['readability'] < 0.7:\n",
    "            recommendations.append(\"Improve code readability by adding more comments and using clearer variable names\")\n",
    "        \n",
    "        if evaluation_results['quality_metrics']['documentation'] < 0.7:\n",
    "            recommendations.append(\"Add more documentation and docstrings to improve code maintainability\")\n",
    "        \n",
    "        if evaluation_results['quality_metrics']['test_coverage'] < 0.7:\n",
    "            recommendations.append(\"Increase test coverage to ensure code reliability\")\n",
    "        \n",
    "        # Functional correctness recommendations\n",
    "        if evaluation_results['functional_correctness']['algorithm_accuracy'] < 0.9:\n",
    "            recommendations.append(\"Improve algorithm accuracy by fixing bugs and edge cases\")\n",
    "        \n",
    "        if evaluation_results['functional_correctness']['edge_case_handling'] < 0.7:\n",
    "            recommendations.append(\"Add better error handling and edge case management\")\n",
    "        \n",
    "        # Performance recommendations\n",
    "        if evaluation_results['performance']['execution_time'] < 0.7:\n",
    "            recommendations.append(\"Optimize code performance for better execution speed\")\n",
    "        \n",
    "        if evaluation_results['performance']['memory_usage'] < 0.7:\n",
    "            recommendations.append(\"Reduce memory usage for better scalability\")\n",
    "        \n",
    "        # Research fidelity recommendations\n",
    "        if evaluation_results['research_fidelity']['algorithm_completeness'] < 0.9:\n",
    "            recommendations.append(\"Ensure all algorithms from the paper are fully implemented\")\n",
    "        \n",
    "        if evaluation_results['research_fidelity']['experimental_reproduction'] < 0.7:\n",
    "            recommendations.append(\"Add experimental setup and evaluation metrics to match the paper\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _identify_strengths(self, evaluation_results):\n",
    "        \"\"\"Identify code strengths\"\"\"\",
    "        strengths = []\n",
    "        \n",
    "        # Check for high-scoring areas\n",
    "        if evaluation_results['quality_metrics']['readability'] >= 0.8:\n",
    "            strengths.append(\"High code readability\")\n",
    "        \n",
    "        if evaluation_results['quality_metrics']['documentation'] >= 0.8:\n",
    "            strengths.append(\"Excellent documentation coverage\")\n",
    "        \n",
    "        if evaluation_results['functional_correctness']['algorithm_accuracy'] >= 0.9:\n",
    "            strengths.append(\"High algorithm accuracy\")\n",
    "        \n",
    "        if evaluation_results['performance']['overall'] >= 0.8:\n",
    "            strengths.append(\"Good performance characteristics\")\n",
    "        \n",
    "        if evaluation_results['research_fidelity']['overall'] >= 0.8:\n",
    "            strengths.append(\"Strong research fidelity\")\n",
    "        \n",
    "        return strengths\n",
    "    \n",
    "    def _identify_weaknesses(self, evaluation_results):\n",
    "        \"\"\"Identify code weaknesses\"\"\"\",
    "        weaknesses = []\n",
    "        \n",
    "        # Check for low-scoring areas\n",
    "        if evaluation_results['quality_metrics']['readability'] < 0.6:\n",
    "            weaknesses.append(\"Low code readability\")\n",
    "        \n",
    "        if evaluation_results['quality_metrics']['documentation'] < 0.6:\n",
    "            weaknesses.append(\"Insufficient documentation\")\n",
    "        \n",
    "        if evaluation_results['functional_correctness']['algorithm_accuracy'] < 0.7:\n",
    "            weaknesses.append(\"Algorithm accuracy needs improvement\")\n",
    "        \n",
    "        if evaluation_results['performance']['overall'] < 0.6:\n",
    "            weaknesses.append(\"Performance issues detected\")\n",
    "        \n",
    "        if evaluation_results['research_fidelity']['overall'] < 0.6:\n",
    "            weaknesses.append(\"Research fidelity needs improvement\")\n",
    "        \n",
    "        return weaknesses\n",
    "\n",
    "print(\"✅ Comprehensive code quality evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationDataGenerator:\n",
    "    \"\"\"Generate evaluation data for testing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sample_paper_data():\n",
    "        \"\"\"Generate sample paper data for evaluation\"\"\"\n",
    "        return {\n",
    "            'title': 'Attention Is All You Need',\n",
    "            'content': '''\n",
    "The dominant sequence transduction models are based on complex recurrent neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "Our model achieves 28.4 BLEU on the English-to-German translation task, improving over the existing best results by over 2 BLEU. On the English-to-French translation task, we establish a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it to English constituency parsing both with large and limited training data.\n",
    "            ''',\n",
    "            'algorithms': [\n",
    "                {\n",
    "                    'name': 'Transformer',\n",
    "                    'description': 'A novel neural network architecture based solely on attention mechanisms',\n",
    "                    'complexity': 'O(n^2) for attention computation',\n",
    "                    'key_parameters': ['d_model', 'n_heads', 'n_layers'],\n",
    "                    'mathematical_formulation': 'Attention(Q,K,V) = softmax(QK^T/√d_k)V',\n",
    "                    'implementation_details': 'Multi-head attention with positional encoding'\n",
    "                }\n",
    "            ],\n",
    "            'experiments': [\n",
    "                {\n",
    "                    'name': 'Machine Translation',\n",
    "                    'description': 'English-to-German translation task',\n",
    "                    'dataset': 'WMT 2014',\n",
    "                    'evaluation_metrics': ['BLEU', 'TER'],\n",
    "                    'baselines': ['LSTM', 'GRU'],\n",
    "                    'results': 'New state-of-the-art results'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sample_code_implementation():\n",
    "        \"\"\"Generate sample code implementation for evaluation\"\"\"\n",
    "        return Mock(\n",
    "            language_used='Python',\n",
    "            framework_used='PyTorch',\n",
    "            generated_files=[\n",
    "                Mock(\n",
    "                    name='transformer.py',\n",
    "                    file_type='main',\n",
    "                    language='Python',\n",
    "                    content='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Linear transformations\n",
    "        Q = self.W_q(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate and final linear transformation\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "''',\n",
    "                    purpose='Main transformer implementation',\n",
    "                    dependencies=['torch'],\n",
    "                    key_functions=['__init__', 'forward']\n",
    "                ),\n",
    "                Mock(\n",
    "                    name='test_transformer.py',\n",
    "                    file_type='test',\n",
    "                    language='Python',\n",
    "                    content='''\n",
    "import unittest\n",
    "import torch\n",
    "from transformer import MultiHeadAttention\n",
    "\n",
    "class TestMultiHeadAttention(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.d_model = 512\n",
    "        self.n_heads = 8\n",
    "        self.attention = MultiHeadAttention(self.d_model, self.n_heads)\n",
    "        \n",
    "    def test_forward_pass(self):\n",
    "        batch_size = 32\n",
    "        seq_len = 100\n",
    "        x = torch.randn(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        output = self.attention(x)\n",
    "        \n",
    "        self.assertEqual(output.shape, (batch_size, seq_len, self.d_model))\n",
    "    \n",
    "    def test_masking(self):\n",
    "        batch_size = 2\n",
    "        seq_len = 10\n",
    "        x = torch.randn(batch_size, seq_len, self.d_model)\n",
    "        mask = torch.ones(batch_size, seq_len, seq_len)\n",
    "        mask[:, 5:, :] = 0  # Mask last 5 tokens\n",
    "        \n",
    "        output = self.attention(x, mask)\n",
    "        \n",
    "        self.assertEqual(output.shape, (batch_size, seq_len, self.d_model))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "''',\n",
    "                    purpose='Test suite for transformer implementation',\n",
    "                    dependencies=['unittest', 'torch'],\n",
    "                    key_functions=['test_forward_pass', 'test_masking']\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sample_test_cases():\n",
    "        \"\"\"Generate sample test cases for evaluation\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'function_name': 'MultiHeadAttention',\n",
    "                'input': 'torch.randn(32, 100, 512)',\n",
    "                'expected': 'torch.randn(32, 100, 512)'\n",
    "            },\n",
    "            {\n",
    "                'function_name': 'MultiHeadAttention',\n",
    "                'input': 'torch.randn(16, 50, 256)',\n",
    "                'expected': 'torch.randn(16, 50, 256)'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sample_test_inputs():\n",
    "        \"\"\"Generate sample test inputs for performance evaluation\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                'function_name': 'MultiHeadAttention',\n",
    "                'input': 'torch.randn(32, 100, 512)',\n",
    "                'size': 100\n",
    "            },\n",
    "            {\n",
    "                'function_name': 'MultiHeadAttention',\n",
    "                'input': 'torch.randn(16, 50, 256)',\n",
    "                'size': 50\n",
    "            },\n",
    "            {\n",
    "                'function_name': 'MultiHeadAttention',\n",
    "                'input': 'torch.randn(8, 25, 128)',\n",
    "                'size': 25\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_sample_expected_patterns():\n",
    "        \"\"\"Generate sample expected patterns for evaluation\"\"\"\n",
    "        return [\n",
    "            'MultiHeadAttention',\n",
    "            'torch',\n",
    "            'nn.Module',\n",
    "            'forward',\n",
    "            'softmax',\n",
    "            'mask',\n",
    "            'linear',\n",
    "            'attention'\n",
    "        ]\n",
    "\n",
    "print(\"✅ Evaluation data generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationVisualizer:\n",
    "    \"\"\"Visualize evaluation results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.figures = {}\n",
    "    \n",
    "    def plot_evaluation_metrics(self, evaluation_results, save_path=None):\n",
    "        \"\"\"Plot evaluation metrics\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Code Quality Evaluation Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Quality metrics\n",
    "        quality_metrics = evaluation_results['quality_metrics']\n",
    "        axes[0, 0].bar(quality_metrics.keys(), quality_metrics.values(), color='skyblue')\n",
    "        axes[0, 0].set_title('Code Quality Metrics')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].set_ylim(0, 1)\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(quality_metrics.values()):\n",
    "            axes[0, 0].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Functional correctness\n",
    "        functional_results = evaluation_results['functional_correctness']\n",
    "        axes[0, 1].bar(functional_results.keys(), functional_results.values(), color='lightgreen')\n",
    "        axes[0, 1].set_title('Functional Correctness')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(functional_results.values()):\n",
    "            axes[0, 1].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Performance metrics\n",
    "        performance_results = evaluation_results['performance']\n",
    "        axes[1, 0].bar(performance_results.keys(), performance_results.values(), color='lightcoral')\n",
    "        axes[1, 0].set_title('Performance Metrics')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].set_ylim(0, 1)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(performance_results.values()):\n",
    "            axes[1, 0].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Research fidelity\n",
    "        research_results = evaluation_results['research_fidelity']\n",
    "        axes[1, 1].bar(research_results.keys(), research_results.values(), color='lightgoldenrodyellow')\n",
    "        axes[1, 1].set_title('Research Fidelity')\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(research_results.values()):\n",
    "            axes[1, 1].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Evaluation metrics plot saved to {save_path}\")\n",
    "        \n",
    "        self.figures['evaluation_metrics'] = fig\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_overall_score(self, evaluation_results, save_path=None):\n",
    "        \"\"\"Plot overall evaluation score\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        overall_score = evaluation_results['overall_score']\n",
    "        grade = evaluation_results.get('grade', 'Unknown')\n",
    "        \n",
    "        # Create a donut chart\n",
    "        sizes = [overall_score, 1 - overall_score]\n",
    "        colors = ['#2ecc71', '#ecf0f1']\n",
    "        \n",
    "        wedges, texts, autotexts = ax.pie(sizes, colors=colors, autopct='%1.1f%%', \n",
    "                                         startangle=90, wedgeprops=dict(width=0.3))\n",
    "        \n",
    "        # Add center text\n",
    "        centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "        fig.gca().add_artist(centre_circle)\n",
    "        \n",
    "        # Add score and grade in center\n",
    "        ax.text(0, 0, f'{overall_score:.2f}\\n{grade}', \n",
    "                ha='center', va='center', fontsize=24, fontweight='bold')\n",
    "        \n",
    "        ax.set_title('Overall Code Quality Score', fontsize=16, fontweight='bold')\n",
    "        ax.axis('equal')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Overall score plot saved to {save_path}\")\n",
    "        \n",
    "        self.figures['overall_score'] = fig\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_evaluation_trends(self, evaluation_history, save_path=None):\n",
    "        \"\"\"Plot evaluation trends over time\"\"\"\n",
    "        if not evaluation_history:\n",
    "            logger.warning(\"No evaluation history provided\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Extract scores over time\n",
    "        timestamps = []\n",
    "        overall_scores = []\n",
    "        quality_scores = []\n",
    "        functional_scores = []\n",
    "        performance_scores = []\n",
    "        research_scores = []\n",
    "        \n",
    "        for eval_result in evaluation_history:\n",
    "            timestamps.append(eval_result['evaluation_timestamp'])\n",
    "            overall_scores.append(eval_result['overall_score'])\n",
    "            quality_scores.append(np.mean(list(eval_result['quality_metrics'].values())))\n",
    "            functional_scores.append(eval_result['functional_correctness']['overall'])\n",
    "            performance_scores.append(eval_result['performance']['overall'])\n",
    "            research_scores.append(eval_result['research_fidelity']['overall'])\n",
    "        \n",
    "        # Convert timestamps to readable format\n",
    "        timestamps = [pd.to_datetime(ts) for ts in timestamps]\n",
    "        \n",
    "        # Plot trends\n",
    "        ax.plot(timestamps, overall_scores, 'o-', label='Overall', linewidth=2, markersize=8)\n",
    "        ax.plot(timestamps, quality_scores, 's-', label='Quality', linewidth=2, markersize=6)\n",
    "        ax.plot(timestamps, functional_scores, '^-', label='Functional', linewidth=2, markersize=6)\n",
    "        ax.plot(timestamps, performance_scores, 'd-', label='Performance', linewidth=2, markersize=6)\n",
    "        ax.plot(timestamps, research_scores, 'v-', label='Research', linewidth=2, markersize=6)\n",
    "        \n",
    "        ax.set_title('Code Quality Evaluation Trends', fontsize=16, fontweight='bold')\n",
    "        ax.set_xlabel('Evaluation Time')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Evaluation trends plot saved to {save_path}\")\n",
    "        \n",
    "        self.figures['evaluation_trends'] = fig\n",
    "        plt.show()\n",
    "    \n",
    "    def save_all_figures(self, output_dir):\n",
    "        \"\"\"Save all generated figures\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for figure_name, figure in self.figures.items():\n",
    "            save_path = output_path / f\"{figure_name}.png\"\n",
    "            figure.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            logger.info(f\"Figure saved to {save_path}\")\n",
    "        \n",
    "        logger.info(f\"All figures saved to {output_path}\")\n",
    "\n",
    "print(\"✅ Evaluation visualizer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Evaluation Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"Run comprehensive code quality evaluation\"\"\"\n",
    "    logger.info(\"Starting comprehensive code quality evaluation...\")\n",
    "    \n",
    "    # Initialize evaluators\n",
    "    evaluator = ComprehensiveCodeQualityEvaluator()\n",
    "    visualizer = EvaluationVisualizer()\n",
    "    data_generator = EvaluationDataGenerator()\n",
    "    \n",
    "    # Generate sample data\n",
    "    logger.info(\"Generating sample evaluation data...\")\n",
    "    paper_data = data_generator.generate_sample_paper_data()\n",
    "    code_implementation = data_generator.generate_sample_code_implementation()\n",
    "    test_cases = data_generator.generate_sample_test_cases()\n",
    "    test_inputs = data_generator.generate_sample_test_inputs()\n",
    "    expected_patterns = data_generator.generate_sample_expected_patterns()\n",
    "    \n",
    "    # Create mock paper object\n",
    "    class MockPaper:\n",
    "        def __init__(self, data):\n",
    "            self.content = data['content']\n",
    "            self.algorithms = data['algorithms']\n",
    "            self.experiments = data['experiments']\n",
    "    \n",
    "    paper = MockPaper(paper_data)\n",
    "    \n",
    "    # Run evaluation\n",
    "    logger.info(\"Running comprehensive evaluation...\")\n",
    "    evaluation_results = evaluator.evaluate_code_quality(\n",
    "        code_implementation, paper, test_cases, expected_patterns, test_inputs\n",
    "    )\n",
    "    \n",
    "    # Generate report\n",
    "    logger.info(\"Generating evaluation report...\")\n",
    "    evaluation_report = evaluator.generate_evaluation_report(evaluation_results)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPREHENSIVE CODE QUALITY EVALUATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n📊 Overall Score: {evaluation_results['overall_score']:.3f}\")\n",
    "    print(f\"📈 Grade: {evaluation_report['evaluation_summary']['grade']}\")\n",
    "    print(f\"🕒 Evaluation Time: {evaluation_report['evaluation_summary']['evaluation_timestamp']}\")\n",
    "    \n",
    "    print(\"\\n🔍 Code Summary:\")\n",
    "    code_summary = evaluation_results['code_summary']\n",
    "    print(f\"   - Total Files: {code_summary['total_files']}\")\n",
    "    print(f\"   - Main Code Lines: {code_summary['main_code_lines']}\")\n",
    "    print(f\"   - Test Code Lines: {code_summary['test_code_lines']}\")\n",
    "    print(f\"   - Language: {code_summary['language']}\")\n",
    "    print(f\"   - Framework: {code_summary['framework']}\")\n",
    "    \n",
    "    print(\"\\n📋 Quality Metrics:\")\n",
    "    for metric, score in evaluation_results['quality_metrics'].items():\n",
    "        print(f\"   - {metric.replace('_', ' ').title()}: {score:.3f}\")\n",
    "    \n",
    "    print(\"\\n✅ Functional Correctness:\")\n",
    "    for metric, score in evaluation_results['functional_correctness'].items():\n",
    "        if metric != 'overall':\n",
    "            print(f\"   - {metric.replace('_', ' ').title()}: {score:.3f}\")\n",
    "    \n",
    "    print(\"\\n⚡ Performance Metrics:\")\n",
    "    for metric, score in evaluation_results['performance'].items():\n",
    "        if metric != 'overall':\n",
    "            print(f\"   - {metric.replace('_', ' ').title()}: {score:.3f}\")\n",
    "    \n",
    "    print(\"\\n🔬 Research Fidelity:\")\n",
    "    for metric, score in evaluation_results['research_fidelity'].items():\n",
    "        if metric != 'overall':\n",
    "            print(f\"   - {metric.replace('_', ' ').title()}: {score:.3f}\")\n",
    "    \n",
    "    print(\"\\n🎯 Strengths:\")\n",
    "    for strength in evaluation_report['strengths']:\n",
    "        print(f\"   - {strength}\")\n",
    "    \n",
    "    print(\"\\n⚠️  Weaknesses:\")\n",
    "    for weakness in evaluation_report['weaknesses']:\n",
    "        print(f\"   - {weakness}\")\n",
    "    \n",
    "    print(\"\\n💡 Recommendations:\")\n",
    "    for i, recommendation in enumerate(evaluation_report['recommendations'], 1):\n",
    "        print(f\"   {i}. {recommendation}\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    logger.info(\"Generating visualizations...\")\n",
    "    output_dir = Path(\"evaluation_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    visualizer.plot_evaluation_metrics(evaluation_results, \n",
    "                                     output_dir / \"evaluation_metrics.png\")\n",
    "    visualizer.plot_overall_score(evaluation_results, \n",
    "                                 output_dir / \"overall_score.png\")\n",
    "    \n",
    "    # Save evaluation report\n",
    "    report_path = output_dir / \"evaluation_report.json\"\n",
    "    with open(report_path, 'w') as f:\n",
    "        json.dump(evaluation_report, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Evaluation report saved to {report_path}\")\n",
    "    logger.info(\"Comprehensive evaluation completed successfully!\")\n",
    "    \n",
    "    return evaluation_results, evaluation_report\n",
    "\n",
    "def run_evaluation_comparison():\n",
    "    \"\"\"Run evaluation comparison with different code samples\"\"\"\n",
    "    logger.info(\"Running evaluation comparison...\")\n",
    "    \n",
    "    evaluator = ComprehensiveCodeQualityEvaluator()\n",
    "    data_generator = EvaluationDataGenerator()\n",
    "    \n",
    "    # Generate different code samples for comparison\n",
    "    code_samples = [\n",
    "        (\"High Quality Code\", data_generator.generate_sample_code_implementation()),\n",
    "        (\"Medium Quality Code\", create_medium_quality_code()),\n",
    "        (\"Low Quality Code\", create_low_quality_code())\n",
    "    ]\n",
    "    \n",
    "    paper_data = data_generator.generate_sample_paper_data()\n",
    "    \n",
    "    class MockPaper:\n",
    "        def __init__(self, data):\n",
    "            self.content = data['content']\n",
    "            self.algorithms = data['algorithms']\n",
    "            self.experiments = data['experiments']\n",
    "    \n",
    "    paper = MockPaper(paper_data)\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for sample_name, code_implementation in code_samples:\n",
    "        logger.info(f\"Evaluating {sample_name}...\")\n",
    "        \n",
    "        result = evaluator.evaluate_code_quality(\n",
    "            code_implementation, paper, [], [], []\n",
    "        )\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            'sample_name': sample_name,\n",
    "            'overall_score': result['overall_score'],\n",
    "            'quality_metrics': result['quality_metrics'],\n",
    "            'functional_correctness': result['functional_correctness'],\n",
    "            'performance': result['performance'],\n",
    "            'research_fidelity': result['research_fidelity']\n",
    "        })\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Code Quality Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    sample_names = [result['sample_name'] for result in evaluation_results]\n",
    "    colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "    \n",
    "    # Overall scores\n",
    "    overall_scores = [result['overall_score'] for result in evaluation_results]\n",
    "    axes[0, 0].bar(sample_names, overall_scores, color=colors)\n",
    "    axes[0, 0].set_title('Overall Scores')\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(overall_scores):\n",
    "        axes[0, 0].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Quality metrics comparison\n",
    "    quality_scores = [np.mean(list(result['quality_metrics'].values())) for result in evaluation_results]\n",
    "    axes[0, 1].bar(sample_names, quality_scores, color=colors)\n",
    "    axes[0, 1].set_title('Quality Metrics')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(quality_scores):\n",
    "        axes[0, 1].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Functional correctness comparison\n",
    "    functional_scores = [result['functional_correctness']['overall'] for result in evaluation_results]\n",
    "    axes[1, 0].bar(sample_names, functional_scores, color=colors)\n",
    "    axes[1, 0].set_title('Functional Correctness')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(functional_scores):\n",
    "        axes[1, 0].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Research fidelity comparison\n",
    "    research_scores = [result['research_fidelity']['overall'] for result in evaluation_results]\n",
    "    axes[1, 1].bar(sample_names, research_scores, color=colors)\n",
    "    axes[1, 1].set_title('Research Fidelity')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, v in enumerate(research_scores):\n",
    "        axes[1, 1].text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_dir = Path(\"evaluation_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    comparison_path = output_dir / \"comparison.png\"\n",
    "    plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "    logger.info(f\"Comparison plot saved to {comparison_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_path = output_dir / \"comparison_results.json\"\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(evaluation_results, f, indent=2, default=str)\n",
    "    \n",
    "    logger.info(f\"Comparison results saved to {comparison_path}\")\n",
    "    logger.info(\"Evaluation comparison completed successfully!\")\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "def create_medium_quality_code():\n",
    "    \"\"\"Create medium quality code for comparison\"\"\"\n",
    "    class MockCodeImplementation:\n",
    "        def __init__(self):\n",
    "            self.language_used = 'Python'\n",
    "            self.framework_used = 'PyTorch'\n",
    "            self.generated_files = [\n",
    "                Mock(\n",
    "                    name='transformer_medium.py',\n",
    "                    file_type='main',\n",
    "                    language='Python',\n",
    "                    content='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "''',\n",
    "                    purpose='Main transformer implementation',\n",
    "                    dependencies=['torch'],\n",
    "                    key_functions=['__init__', 'forward']\n",
    "                )\n",
    "            ]\n",
    "    \n",
    "    return MockCodeImplementation()\n",
    "\n",
    "def create_low_quality_code():\n",
    "    \"\"\"Create low quality code for comparison\"\"\"\n",
    "    class MockCodeImplementation:\n",
    "        def __init__(self):\n",
    "            self.language_used = 'Python'\n",
    "            self.framework_used = 'PyTorch'\n",
    "            self.generated_files = [\n",
    "                Mock(\n",
    "                    name='transformer_bad.py',\n",
    "                    file_type='main',\n",
    "                    language='Python',\n",
    "                    content='''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "''',\n",
    "                    purpose='Main transformer implementation',\n",
    "                    dependencies=['torch'],\n",
    "                    key_functions=['__init__', 'forward']\n",
    "                )\n",
    "            ]\n",
    "    \n",
    "    return MockCodeImplementation()\n",
    "\n",
    "print(\"✅ Evaluation workflow initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "evaluation_results, evaluation_report = run_comprehensive_evaluation()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation comparison\n",
    "comparison_results = run_evaluation_comparison()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "output_dir = Path(\"evaluation_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save main evaluation results\n",
    "with open(output_dir / \"main_evaluation.json\", 'w') as f:\n",
    "    json.dump(evaluation_report, f, indent=2, default=str)\n",
    "\n",
    "# Save comparison results\n",
    "with open(output_dir / \"comparison_results.json\", 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Evaluation results saved to {output_dir}\")\n",
    "print(f\"📊 Main evaluation: {output_dir / 'main_evaluation.json'}\")\n",
    "print(f\"📈 Comparison results: {output_dir / 'comparison_results.json'}\")\n",
    "print(f\"🖼️  Visualizations: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This comprehensive evaluation framework provides:\n",
    "\n",
    "1. **Code Quality Metrics**: Readability, complexity, documentation, test coverage\n",
    "2. **Functional Correctness**: Algorithm accuracy, implementation correctness, edge case handling\n",
    "3. **Performance Metrics**: Execution time, memory usage, scalability\n",
    "4. **Research Fidelity**: Algorithm completeness, paper-to-code accuracy, experimental reproduction\n",
    "\n",
    "The evaluation system generates detailed reports with:\n",
    "- Overall quality scores and grades\n",
    "- Individual metric breakdowns\n",
    "- Strengths and weaknesses identification\n",
    "- Actionable recommendations for improvement\n",
    "- Visualizations for better understanding\n",
    "\n",
    "The framework is designed to be extensible and can be easily integrated with the Paper2Code agent system for continuous quality assessment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}